{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import copy as cp\n",
    "import pandas as pd\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os.path\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get some of the TensorFlow specific items out of the way first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'model_dir', '/tmp/imagenet',\n",
    "    \"\"\"Path to classify_image_graph_def.pb, \"\"\"\n",
    "    \"\"\"imagenet_synset_to_human_label_map.txt, and \"\"\"\n",
    "    \"\"\"imagenet_2012_challenge_label_map_proto.pbtxt.\"\"\")\n",
    "tf.app.flags.DEFINE_string('image_file', '',\n",
    "                           \"\"\"Absolute path to image file.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('num_top_predictions', 5,\n",
    "                            \"\"\"Display this many predictions.\"\"\")\n",
    "\n",
    "# pylint: disable=line-too-long\n",
    "DATA_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "# pylint: enable=line-too-long\n",
    "\n",
    "class NodeLookup(object):\n",
    "  \"\"\"Converts integer node ID's to human readable labels.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               label_lookup_path=None,\n",
    "               uid_lookup_path=None):\n",
    "    if not label_lookup_path:\n",
    "      label_lookup_path = os.path.join(\n",
    "          FLAGS.model_dir, 'imagenet_2012_challenge_label_map_proto.pbtxt')\n",
    "    if not uid_lookup_path:\n",
    "      uid_lookup_path = os.path.join(\n",
    "          FLAGS.model_dir, 'imagenet_synset_to_human_label_map.txt')\n",
    "    self.node_lookup = self.load(label_lookup_path, uid_lookup_path)\n",
    "\n",
    "  def load(self, label_lookup_path, uid_lookup_path):\n",
    "    \"\"\"Loads a human readable English name for each softmax node.\n",
    "\n",
    "    Args:\n",
    "      label_lookup_path: string UID to integer node ID.\n",
    "      uid_lookup_path: string UID to human-readable string.\n",
    "\n",
    "    Returns:\n",
    "      dict from integer node ID to human-readable string.\n",
    "    \"\"\"\n",
    "    if not tf.gfile.Exists(uid_lookup_path):\n",
    "      tf.logging.fatal('File does not exist %s', uid_lookup_path)\n",
    "    if not tf.gfile.Exists(label_lookup_path):\n",
    "      tf.logging.fatal('File does not exist %s', label_lookup_path)\n",
    "\n",
    "    # Loads mapping from string UID to human-readable string\n",
    "    proto_as_ascii_lines = tf.gfile.GFile(uid_lookup_path).readlines()\n",
    "    uid_to_human = {}\n",
    "    p = re.compile(r'[n\\d]*[ \\S,]*')\n",
    "    for line in proto_as_ascii_lines:\n",
    "      parsed_items = p.findall(line)\n",
    "      uid = parsed_items[0]\n",
    "      human_string = parsed_items[2]\n",
    "      uid_to_human[uid] = human_string\n",
    "\n",
    "    # Loads mapping from string UID to integer node ID.\n",
    "    node_id_to_uid = {}\n",
    "    proto_as_ascii = tf.gfile.GFile(label_lookup_path).readlines()\n",
    "    for line in proto_as_ascii:\n",
    "      if line.startswith('  target_class:'):\n",
    "        target_class = int(line.split(': ')[1])\n",
    "      if line.startswith('  target_class_string:'):\n",
    "        target_class_string = line.split(': ')[1]\n",
    "        node_id_to_uid[target_class] = target_class_string[1:-2]\n",
    "\n",
    "    # Loads the final mapping of integer node ID to human-readable string\n",
    "    node_id_to_name = {}\n",
    "    for key, val in node_id_to_uid.items():\n",
    "      if val not in uid_to_human:\n",
    "        tf.logging.fatal('Failed to locate: %s', val)\n",
    "      name = uid_to_human[val]\n",
    "      node_id_to_name[key] = name\n",
    "\n",
    "    return node_id_to_name\n",
    "\n",
    "  def id_to_string(self, node_id):\n",
    "    if node_id not in self.node_lookup:\n",
    "      return ''\n",
    "    return self.node_lookup[node_id]\n",
    "\n",
    "def create_graph():\n",
    "  \"\"\"Creates a graph from saved GraphDef file and returns a saver.\"\"\"\n",
    "  # Creates graph from saved graph_def.pb.\n",
    "  with tf.gfile.FastGFile(os.path.join(\n",
    "      FLAGS.model_dir, 'classify_image_graph_def.pb'), 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    _ = tf.import_graph_def(graph_def, name='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def maybe_download_and_extract():\n",
    "    \"\"\"Download and extract model tar file.\"\"\"\n",
    "    \n",
    "    dest_directory = FLAGS.model_dir\n",
    "    if not os.path.exists(dest_directory):\n",
    "        os.makedirs(dest_directory)\n",
    "    \n",
    "    filename = DATA_URL.split('/')[-1]\n",
    "    filepath = os.path.join(dest_directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\r>> Downloading %s %.1f%%' % (\n",
    "                    filename, float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath,\n",
    "                                                 reporthook=_progress)\n",
    "        print()\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Succesfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "    \n",
    "    tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n",
    "\n",
    "maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to create a generator over images\n",
    "def dataset_gen(directory):\n",
    "    for name in os.listdir(directory):\n",
    "        full_path = os.path.join(directory, name)\n",
    "        if os.path.isfile(full_path):\n",
    "            yield (os.path.splitext(name)[0], open(full_path, 'rb').read())\n",
    "        else:\n",
    "            print('Unidentified name %s. It could be a symbolic link' % full_path)\n",
    "\n",
    "\n",
    "    \n",
    "def getImage(data_dir, im):\n",
    "    jpg_filepath = os.path.join(data_dir, '%s.jpg' % im) \n",
    "    png_filepath = os.path.join(data_dir, '%s.png' % im)\n",
    "    if os.path.exists(jpg_filepath):\n",
    "        return open(jpg_filepath, 'rb').read()\n",
    "    elif os.path.exists(png_filepath):\n",
    "        return open(png_filepath, 'rb').read()\n",
    "    else:\n",
    "        raise IOError('No file %s.{jpg, png} found in %s' % (im, data_dir))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to generate features\n",
    "def generateFeatures(layer_name, dataset):\n",
    "    \"\"\"Generate and save features as csv for a particular layer and dataset.\n",
    "    Keyword arguments:\n",
    "    layer_name -- String: the name of the tensor, ex 'pool_3:0'\n",
    "    dataset -- Generator: an iterator over the image dataset\n",
    "    \"\"\" \n",
    "    create_graph()\n",
    "    all_features = {}\n",
    "    with tf.Session() as sess:\n",
    "        layer = sess.graph.get_tensor_by_name(layer_name)\n",
    "        for (rec_id,image_data) in dataset:\n",
    "            try:\n",
    "                features = sess.run(layer, {'DecodeJpeg/contents:0': image_data})\n",
    "                features = np.reshape(features, (np.product(features.shape)))\n",
    "                all_features[rec_id] = features\n",
    "            except Exception as e:\n",
    "                print(\"Error for \",rec_id)\n",
    "    return all_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for  33092\n",
      "Unidentified name web_data/img/untitled folder. It could be a symbolic link\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset_gen(\"web_data/img/\")\n",
    "features = generateFeatures('pool_3:0',dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    }
   ],
   "source": [
    "print(len(features.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump( features, open( \"web_data/data/features.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9413\n"
     ]
    }
   ],
   "source": [
    "#get the captions\n",
    "recs = pickle.load(open(\"test_data/caption/recipes19.p\",\"rb\"))\n",
    "print(len(recs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_caps = {}\n",
    "for k in features.keys():\n",
    "    if k in recs:\n",
    "        test_caps[k]=recs[k]\n",
    "    else:\n",
    "        print(\"not in recs\",k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "print(len(test_caps.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_feats={}\n",
    "for k in test_caps.keys():\n",
    "    test_feats[k]=features[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump( test_feats, open( \"test_data/features/features.p\", \"wb\" ) )\n",
    "pickle.dump( test_caps, open( \"test_data/caption/caption.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
