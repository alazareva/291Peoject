{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.web import URL, DOM, plaintext, Element, extension, Crawler, DEPTH\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove dummy images from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy = Image.open(\"img/9908.png\").histogram() #insert new dummy image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Error with file', '.DS_Store', IOError(\"cannot identify image file 'img/.DS_Store'\",))\n",
      "('dummy', '9055.png')\n",
      "('dummy', '91687.png')\n",
      "('dummy', '9681.png')\n",
      "('dummy', '9683.png')\n",
      "('dummy', '97589.png')\n",
      "('dummy', '98879.png')\n",
      "('dummy', '9908.png')\n"
     ]
    }
   ],
   "source": [
    "indir = 'img/'\n",
    "dummies = []\n",
    "for root, dirs, filenames in os.walk(indir):\n",
    "    for f in filenames:\n",
    "        try:\n",
    "            img = Image.open(indir + f).histogram()\n",
    "            if img == dummy:\n",
    "                print(\"dummy\",f)\n",
    "                dummies.append(os.path.basename(f))\n",
    "                os.remove(indir + f)\n",
    "        except Exception as detail:\n",
    "            print(\"Error with file\",f,detail)\n",
    "pickle.dump( dummies, open( \"dummies.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('orig size', 5000)\n",
      "('dummy size', 293)\n",
      "('new size', 4889)\n"
     ]
    }
   ],
   "source": [
    "with open('dummies.csv') as f:\n",
    "    dummies = f.read().splitlines()\n",
    "    recipe_list = pickle.load( open( \"recipe_lists/recipe_list2016-04-22.p\", \"rb\" ))\n",
    "    print(\"orig size\", len(recipe_list.keys() ))\n",
    "    print(\"dummy size\", len(dummies))\n",
    "    recipe_list_new = {key: value for key, value in recipe_list.items() \n",
    "             if key not in dummies}\n",
    "    print(\"new size\", len(recipe_list_new.keys() ))\n",
    "    pickle.dump( recipe_list_new, open( \"recipe_lists/recipe_list2016-04-25_remove_dummies.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4707\n"
     ]
    }
   ],
   "source": [
    "print(5000-293) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Scraper():\n",
    "    def save_image(self):\n",
    "        pass\n",
    "    def get_recipe(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AllRecipesScraper(Scraper):    \n",
    "        \n",
    "    def get_ingredients(self,element):\n",
    "        ing_nodes = element.by_class(\"recipe-ingred_txt added\")\n",
    "        return \"\\n\".join([plaintext(a.content) for a in ing_nodes])\n",
    "\n",
    "    \n",
    "    def get_instructions(self,element):\n",
    "        instr_nodes = element.by_class(\"recipe-directions__list--item\")\n",
    "        return \"\\n\".join([plaintext(a.content) for a in instr_nodes])\n",
    "    \n",
    "    def get_recipe(self,element):\n",
    "        return self.get_ingredients(element)+\"\\n\"+self.get_instructions(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AllRecipesRandomSearch():\n",
    "    def __init__(self,tried_ids = None, recipe_list = None ):\n",
    "        self.scraper = AllRecipesScraper()\n",
    "        if tried_ids is None:\n",
    "            self.tried_ids = set()\n",
    "        else:\n",
    "            self.tried_ids = tried_ids\n",
    "        if recipe_list is None:\n",
    "            self.recipe_list = {}\n",
    "        else:\n",
    "            self.recipe_list = recipe_list\n",
    "        self.count = 0\n",
    "        \n",
    "    def reset_count(self):\n",
    "        self.count = 0\n",
    "        \n",
    "    def new_id(self,rec_id):\n",
    "        return rec_id not in self.tried_ids\n",
    "          \n",
    "    def visit(self,rec_id):\n",
    "        url = URL(\"http://allrecipes.com/recipe/\"+str(rec_id))\n",
    "        try:\n",
    "            source = url.download(cached=True)\n",
    "            self.scrape(source, rec_id)\n",
    "        except Exception as detail:\n",
    "            print 'Unable to Scrape:', rec_id\n",
    "            self.tried_ids.add(rec_id)\n",
    "            \n",
    "    def scrape(self,source,rec_id):\n",
    "        print(\"scraping\", rec_id)\n",
    "        element = Element(source)\n",
    "        recipe = self.scraper.get_recipe(element)\n",
    "        self.recipe_list[rec_id]=recipe\n",
    "        self.tried_ids.add(rec_id)\n",
    "        self.count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get recipes for images that were not in the list but saved on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a01dbe8c6b61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrecipe_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"recipe_lists/recipe_list2016-04-25_remove_dummies.p\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecipe_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"keys size\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "recipe_list = pickle.load(open(\"recipe_lists/recipe_list2016-04-25_remove_dummies.p\",\"rb\"))\n",
    "keys = recipe_list.keys()\n",
    "\n",
    "print(\"keys size\", len(keys))\n",
    "\n",
    "search = AllRecipesRandomSearch(recipe_list = recipe_list)\n",
    "for root, dirs, filenames in os.walk(indir):\n",
    "    for f in filenames:\n",
    "        base = os.path.basename(f)\n",
    "        rec_id = os.path.splitext(base)[0]\n",
    "        if rec_id not in keys:\n",
    "            print(\"Getting recipe for\",rec_id)\n",
    "            search.visit(rec_id)\n",
    "            time.sleep(1)\n",
    "\n",
    "save_as = \"recipe_lists/recipe_list\"+str(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\"))+\".p\"\n",
    "pickle.dump( crawler.recipe_list, open(save_as , \"wb\" ) ) \n",
    "print \"Saved as\", save_as\n",
    "print(\"new Keys size\",len(crawler.recipe_list.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the final work\n",
      "('new Keys size', 8647)\n"
     ]
    }
   ],
   "source": [
    "save_as_list = \"recipe_lists/recipe_list_AL.p\"\n",
    "save_as_ids = \"recipe_lists/tried_ids_AL.p\"\n",
    "print \"Saving the final work\"\n",
    "pickle.dump( search.recipe_list, open(save_as_list, \"wb\" ) )\n",
    "pickle.dump( search.tried_ids, open(save_as_ids, \"wb\" ) )\n",
    "print(\"new Keys size\",len(search.recipe_list.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
