{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36377\n"
     ]
    }
   ],
   "source": [
    "all_features = pickle.load(open('master_features_filtered.p', 'rb'))\n",
    "print(len(all_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n",
      "36282\n"
     ]
    }
   ],
   "source": [
    "#Remove the test data that AL created\n",
    "al_test_features = pickle.load(open('test_features.p', 'rb'))\n",
    "print(len(al_test_features))\n",
    "\n",
    "for key in al_test_features.keys():\n",
    "    if key in all_features.keys():\n",
    "        del all_features[key]\n",
    "\n",
    "print(len(all_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10493\n"
     ]
    }
   ],
   "source": [
    "paul_features = pickle.load(open('paul_features.p', 'rb'))\n",
    "print(len(paul_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2522\n"
     ]
    }
   ],
   "source": [
    "set1  = set(paul_features.keys())\n",
    "set2  = set(all_features.keys())\n",
    "c = set1.intersection(set2)\n",
    "print(len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44253\n"
     ]
    }
   ],
   "source": [
    "paul_features.update(all_features)\n",
    "print(len(paul_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36377\n",
      "7490\n",
      "43867\n"
     ]
    }
   ],
   "source": [
    "#Get all the recipes\n",
    "all_recipes = pickle.load(open('new_master_recipe.p', 'rb'))\n",
    "new_recipes = pickle.load(open('hunted_recipes2.p', 'rb'))\n",
    "\n",
    "print(len(all_recipes))\n",
    "print(len(new_recipes))\n",
    "\n",
    "all_recipes.update(new_recipes)\n",
    "print(len(all_recipes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43867\n",
      "43867\n"
     ]
    }
   ],
   "source": [
    "new_dict = dict()\n",
    "\n",
    "#Remove something\n",
    "for key in all_recipes.keys():\n",
    "    value = all_recipes[key]\n",
    "    value.replace(\"Add all ingredients to list\\n\",\"\")\n",
    "    new_dict[key] = value\n",
    "\n",
    "print(len(new_dict))\n",
    "print(len(all_recipes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43798\n",
      "43798\n"
     ]
    }
   ],
   "source": [
    "image_keys = paul_features.keys()\n",
    "recipe_keys = all_recipes.keys()\n",
    "\n",
    "count = 0\n",
    "\n",
    "for key in image_keys:\n",
    "    if key not in recipe_keys:\n",
    "        del paul_features[key]\n",
    "\n",
    "for key in recipe_keys:\n",
    "    if key not in image_keys:\n",
    "        del new_dict[key]\n",
    "\n",
    "print(len(paul_features))\n",
    "print(len(new_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#Cross verify that they match with image feature ids\n",
    "set1 = set(paul_features.keys())\n",
    "set2 = set(new_dict.keys())\n",
    "print(len(set1.symmetric_difference(set2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000\n",
      "4399\n",
      "4529\n"
     ]
    }
   ],
   "source": [
    "#Split the dataset into training set, validation set and test set\n",
    "\n",
    "d1 = dict(paul_features.items()[35000:])\n",
    "train = dict(paul_features.items()[:35000])\n",
    "\n",
    "print(len(train))\n",
    "\n",
    "val = dict(d1.items()[4399:])\n",
    "test = dict(d1.items()[:4399])\n",
    "test.update(al_test_features)\n",
    "print(len(val))\n",
    "print(len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(train, open( \"training_set_features.p\", \"wb\" ) )\n",
    "pickle.dump(val, open( \"validation_set_features.p\", \"wb\" ) )\n",
    "pickle.dump(test, open( \"test_set_features.p\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000\n",
      "4399\n",
      "4399\n"
     ]
    }
   ],
   "source": [
    "val_recipe = dict()\n",
    "train_recipe = dict()\n",
    "test_recipe = dict()\n",
    "\n",
    "for key in paul_features.keys():\n",
    "    if key in train.keys():\n",
    "        train_recipe[key] = new_dict[key]\n",
    "    elif key in val.keys():\n",
    "        val_recipe[key] = new_dict[key]\n",
    "    elif key in test.keys():\n",
    "        test_recipe[key] = new_dict[key]\n",
    "        \n",
    "print(len(train_recipe))\n",
    "print(len(val_recipe))\n",
    "print(len(test_recipe))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(train_recipe, open( \"training_set_recipes.p\", \"wb\" ) )\n",
    "pickle.dump(val_recipe, open( \"validation_set_recipes.p\", \"wb\" ) )\n",
    "pickle.dump(test_recipe, open( \"test_set_recipes.p\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length and shape of training before PCA\n",
      "35000\n",
      "2048\n",
      "The length and shape of test before PCA\n",
      "4399\n",
      "2048\n",
      "The length and shape of val before PCA\n",
      "4399\n",
      "2048\n",
      "35000\n",
      "4399\n",
      "4399\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"The length and shape of training before PCA\")\n",
    "print(len(train))\n",
    "print(len(train.itervalues().next()))\n",
    "print(\"The length and shape of test before PCA\")\n",
    "print(len(test))\n",
    "print(len(test.itervalues().next()))\n",
    "\n",
    "print(\"The length and shape of val before PCA\")\n",
    "print(len(val))\n",
    "print(len(val.itervalues().next()))\n",
    "\n",
    "pca = PCA(copy=True, n_components=512, whiten=True)\n",
    "pca_train_features = pca.fit_transform(np.asarray(train.values()))\n",
    "pca_test_features =  pca.transform(np.asarray(test.values()))\n",
    "pca_val_features =  pca.transform(np.asarray(val.values()))\n",
    "\n",
    "print(len(pca_train_features))\n",
    "print(len(pca_test_features))\n",
    "print(len(pca_val_features))\n",
    "\n",
    "print(type(pca_train_features))\n",
    "print(type(pca_test_features))\n",
    "print(type(pca_val_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000L, 512L)\n",
      "(4399L, 512L)\n",
      "(4399L, 512L)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(pca_train_features))\n",
    "print(np.shape(pca_test_features))\n",
    "print(np.shape(pca_val_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(pca_train_features, open( \"pca_train_features.p\", \"wb\" ) )\n",
    "pickle.dump(pca_test_features, open( \"pca_test_features.p\", \"wb\" ) )\n",
    "pickle.dump(pca_val_features, open( \"pca_val_features.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
