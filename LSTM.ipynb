{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import math\n",
    "import os\n",
    "#import ipdb\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "import tensorflow.python.platform\n",
    "from keras.preprocessing import sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Caption_Generator():\n",
    "\n",
    "    def init_weight(self, dim_in, dim_out, name=None, stddev=1.0):\n",
    "        return tf.Variable(tf.truncated_normal([dim_in, dim_out], stddev=stddev/math.sqrt(float(dim_in))), name=name)\n",
    "\n",
    "    def init_bias(self, dim_out, name=None):\n",
    "        return tf.Variable(tf.zeros([dim_out]), name=name)\n",
    "\n",
    "    def __init__(self, n_words, dim_embed, dim_ctx, dim_hidden, n_lstm_steps, batch_size=200, ctx_shape=[196,512], bias_init_vector=None):\n",
    "        self.n_words = n_words\n",
    "        self.dim_embed = dim_embed\n",
    "        self.dim_ctx = dim_ctx\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.ctx_shape = ctx_shape\n",
    "        self.n_lstm_steps = n_lstm_steps\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.Wemb = tf.Variable(tf.random_uniform([n_words, dim_embed], -1.0, 1.0), name='Wemb')\n",
    "\n",
    "        self.init_hidden_W = self.init_weight(dim_ctx, dim_hidden, name='init_hidden_W')\n",
    "        self.init_hidden_b = self.init_bias(dim_hidden, name='init_hidden_b')\n",
    "\n",
    "        self.init_memory_W = self.init_weight(dim_ctx, dim_hidden, name='init_memory_W')\n",
    "        self.init_memory_b = self.init_bias(dim_hidden, name='init_memory_b')\n",
    "\n",
    "        self.lstm_W = self.init_weight(dim_embed, dim_hidden*4, name='lstm_W')\n",
    "        self.lstm_U = self.init_weight(dim_hidden, dim_hidden*4, name='lstm_U')\n",
    "        self.lstm_b = self.init_bias(dim_hidden*4, name='lstm_b')\n",
    "\n",
    "        self.image_encode_W = self.init_weight(dim_ctx, dim_hidden*4, name='image_encode_W')\n",
    "\n",
    "        self.image_att_W = self.init_weight(dim_ctx, dim_ctx, name='image_att_W')\n",
    "        self.hidden_att_W = self.init_weight(dim_hidden, dim_ctx, name='hidden_att_W')\n",
    "        self.pre_att_b = self.init_bias(dim_ctx, name='pre_att_b')\n",
    "\n",
    "        self.att_W = self.init_weight(dim_ctx, 1, name='att_W')\n",
    "        self.att_b = self.init_bias(1, name='att_b')\n",
    "\n",
    "        self.decode_lstm_W = self.init_weight(dim_hidden, dim_embed, name='decode_lstm_W')\n",
    "        self.decode_lstm_b = self.init_bias(dim_embed, name='decode_lstm_b')\n",
    "\n",
    "        self.decode_word_W = self.init_weight(dim_embed, n_words, name='decode_word_W')\n",
    "\n",
    "        if bias_init_vector is not None:\n",
    "            self.decode_word_b = tf.Variable(bias_init_vector.astype(np.float32), name='decode_word_b')\n",
    "        else:\n",
    "            self.decode_word_b = self.init_bias(n_words, name='decode_word_b')\n",
    "\n",
    "\n",
    "    def get_initial_lstm(self, mean_context):\n",
    "        initial_hidden = tf.nn.tanh(tf.matmul(mean_context, self.init_hidden_W) + self.init_hidden_b)\n",
    "        initial_memory = tf.nn.tanh(tf.matmul(mean_context, self.init_memory_W) + self.init_memory_b)\n",
    "\n",
    "        return initial_hidden, initial_memory\n",
    "\n",
    "    def build_model(self):\n",
    "        context = tf.placeholder(\"float32\", [self.batch_size, self.ctx_shape[0], self.ctx_shape[1]])\n",
    "        sentence = tf.placeholder(\"int32\", [self.batch_size, self.n_lstm_steps])\n",
    "        mask = tf.placeholder(\"float32\", [self.batch_size, self.n_lstm_steps])\n",
    "\n",
    "        h, c = self.get_initial_lstm(tf.reduce_mean(context, 1))\n",
    "\n",
    "        # TensorFlow가 dot(3D tensor, matrix) 계산을 못함;;; ㅅㅂ 삽질 ㄱㄱ\n",
    "        context_flat = tf.reshape(context, [-1, self.dim_ctx])\n",
    "        context_encode = tf.matmul(context_flat, self.image_att_W) # (batch_size, 196, 512)\n",
    "        context_encode = tf.reshape(context_encode, [-1, ctx_shape[0], ctx_shape[1]])\n",
    "\n",
    "        loss = 0.0\n",
    "\n",
    "        for ind in range(self.n_lstm_steps):\n",
    "\n",
    "            if ind == 0:\n",
    "                word_emb = tf.zeros([self.batch_size, self.dim_embed])\n",
    "            else:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "                with tf.device(\"/cpu:0\"):\n",
    "                    word_emb = tf.nn.embedding_lookup(self.Wemb, sentence[:,ind-1])\n",
    "\n",
    "            x_t = tf.matmul(word_emb, self.lstm_W) + self.lstm_b # (batch_size, hidden*4)\n",
    "\n",
    "            labels = tf.expand_dims(sentence[:,ind], 1)\n",
    "            indices = tf.expand_dims(tf.range(0, self.batch_size, 1), 1)\n",
    "            concated = tf.concat(1, [indices, labels])\n",
    "            onehot_labels = tf.sparse_to_dense( concated, tf.pack([self.batch_size, self.n_words]), 1.0, 0.0)\n",
    "\n",
    "            context_encode = context_encode + \\\n",
    "                 tf.expand_dims(tf.matmul(h, self.hidden_att_W), 1) + \\\n",
    "                 self.pre_att_b\n",
    "\n",
    "            context_encode = tf.nn.tanh(context_encode)\n",
    "\n",
    "            # 여기도 context_encode: 3D -> flat required\n",
    "            context_encode_flat = tf.reshape(context_encode, [-1, self.dim_ctx]) # (batch_size*196, 512)\n",
    "            alpha = tf.matmul(context_encode_flat, self.att_W) + self.att_b # (batch_size*196, 1)\n",
    "            alpha = tf.reshape(alpha, [-1, self.ctx_shape[0]])\n",
    "            alpha = tf.nn.softmax( alpha )\n",
    "\n",
    "            weighted_context = tf.reduce_sum(context * tf.expand_dims(alpha, 2), 1)\n",
    "\n",
    "            lstm_preactive = tf.matmul(h, self.lstm_U) + x_t + tf.matmul(weighted_context, self.image_encode_W)\n",
    "            i, f, o, new_c = tf.split(1, 4, lstm_preactive)\n",
    "\n",
    "            i = tf.nn.sigmoid(i)\n",
    "            f = tf.nn.sigmoid(f)\n",
    "            o = tf.nn.sigmoid(o)\n",
    "            new_c = tf.nn.tanh(new_c)\n",
    "\n",
    "            c = f * c + i * new_c\n",
    "            h = o * tf.nn.tanh(new_c)\n",
    "\n",
    "            logits = tf.matmul(h, self.decode_lstm_W) + self.decode_lstm_b\n",
    "            logits = tf.nn.relu(logits)\n",
    "            logits = tf.nn.dropout(logits, 0.5)\n",
    "\n",
    "            logit_words = tf.matmul(logits, self.decode_word_W) + self.decode_word_b\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logit_words, onehot_labels)\n",
    "            cross_entropy = cross_entropy * mask[:,ind]\n",
    "\n",
    "            current_loss = tf.reduce_sum(cross_entropy)\n",
    "            loss = loss + current_loss\n",
    "\n",
    "        loss = loss / tf.reduce_sum(mask)\n",
    "        return loss, context, sentence, mask\n",
    "\n",
    "    def build_generator(self, maxlen):\n",
    "        context = tf.placeholder(\"float32\", [1, self.ctx_shape[0], self.ctx_shape[1]])\n",
    "        h, c = self.get_initial_lstm(tf.reduce_mean(context, 1))\n",
    "        \n",
    "        #print(context[0].shape)\n",
    "        #context_encode = tf.matmul(context[0], self.image_att_W)\n",
    "        context_encode = tf.matmul(tf.squeeze(context,[0]), self.image_att_W)\n",
    "        generated_words = []\n",
    "        logit_list = []\n",
    "        alpha_list = []\n",
    "        word_emb = tf.zeros([1, self.dim_embed])\n",
    "        for ind in range(maxlen):\n",
    "            x_t = tf.matmul(word_emb, self.lstm_W) + self.lstm_b\n",
    "            context_encode = context_encode + tf.matmul(h, self.hidden_att_W) + self.pre_att_b\n",
    "            context_encode = tf.nn.tanh(context_encode)\n",
    "\n",
    "            alpha = tf.matmul(context_encode, self.att_W) + self.att_b\n",
    "            alpha = tf.reshape(alpha, [-1, self.ctx_shape[0]] )\n",
    "            alpha = tf.nn.softmax(alpha)\n",
    "\n",
    "            alpha = tf.reshape(alpha, (ctx_shape[0], -1))\n",
    "            alpha_list.append(alpha)\n",
    "\n",
    "            weighted_context = tf.reduce_sum(tf.squeeze(context) * alpha, 0)\n",
    "            weighted_context = tf.expand_dims(weighted_context, 0)\n",
    "\n",
    "            lstm_preactive = tf.matmul(h, self.lstm_U) + x_t + tf.matmul(weighted_context, self.image_encode_W)\n",
    "\n",
    "            i, f, o, new_c = tf.split(1, 4, lstm_preactive)\n",
    "\n",
    "            i = tf.nn.sigmoid(i)\n",
    "            f = tf.nn.sigmoid(f)\n",
    "            o = tf.nn.sigmoid(o)\n",
    "            new_c = tf.nn.tanh(new_c)\n",
    "\n",
    "            c = f*c + i*new_c\n",
    "            h = o*tf.nn.tanh(new_c)\n",
    "\n",
    "            logits = tf.matmul(h, self.decode_lstm_W) + self.decode_lstm_b\n",
    "            logits = tf.nn.relu(logits)\n",
    "\n",
    "            logit_words = tf.matmul(logits, self.decode_word_W) + self.decode_word_b\n",
    "\n",
    "            max_prob_word = tf.argmax(logit_words, 1)\n",
    "\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                word_emb = tf.nn.embedding_lookup(self.Wemb, max_prob_word)\n",
    "\n",
    "            generated_words.append(max_prob_word)\n",
    "            logit_list.append(logit_words)\n",
    "\n",
    "        return context, generated_words, logit_list, alpha_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### 학습 관련 Parameters ######\n",
    "n_epochs=20\n",
    "batch_size=10\n",
    "dim_embed=256\n",
    "dim_ctx=2048\n",
    "dim_hidden=256\n",
    "ctx_shape=[1,2048]\n",
    "#############################\n",
    "###### 잡다한 Parameters #####\n",
    "annotation_path = 'test_data/caption/caption.p'\n",
    "feat_path = 'test_data/features/features.p'\n",
    "model_path = 'test_data/model/'\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def preProBuildWordVocab(sentence_iterator, word_count_threshold=0): # borrowed this function from NeuralTalk\n",
    "    print 'preprocessing word counts and creating vocab based on word count threshold %d' % (word_count_threshold, )\n",
    "    word_counts = {}\n",
    "    nsents = 0\n",
    "    for sent in sentence_iterator:\n",
    "        nsents += 1\n",
    "        for w in sent.lower().split(' '):\n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "    print 'filtered words from %d to %d' % (len(word_counts), len(vocab))\n",
    "\n",
    "    ixtoword = {}\n",
    "    ixtoword[0] = '#END#'  # end token at the end of the sentence. make first dimension be end token\n",
    "    wordtoix = {}\n",
    "    wordtoix['#START#'] = 0 # make first vector be the start token\n",
    "    ix = 1\n",
    "    for w in vocab:\n",
    "        wordtoix[w] = ix\n",
    "        ixtoword[ix] = w\n",
    "        ix += 1\n",
    "\n",
    "    word_counts['#END#'] = nsents\n",
    "    bias_init_vector = np.array([1.0*word_counts[ixtoword[i]] for i in ixtoword])\n",
    "    bias_init_vector /= np.sum(bias_init_vector) # normalize to frequencies\n",
    "    bias_init_vector = np.log(bias_init_vector)\n",
    "    bias_init_vector -= np.max(bias_init_vector) # shift to nice numeric range\n",
    "    return wordtoix, ixtoword, bias_init_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(pretrained_model_path=None): # 전에 학습하던게 있으면 초기값 설정.\n",
    "    annotation_data = pickle.load(open(annotation_path,\"rb\"))\n",
    "    captions = annotation_data.values()\n",
    "    wordtoix, ixtoword, bias_init_vector = preProBuildWordVocab(captions)\n",
    "\n",
    "    learning_rate=0.001\n",
    "    n_words = len(wordtoix)\n",
    "    feats = np.load(feat_path)\n",
    "    maxlen = np.max( map(lambda x: len(x.split(' ')), captions) )\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "\n",
    "    caption_generator = Caption_Generator(\n",
    "            n_words=n_words,\n",
    "            dim_embed=dim_embed,\n",
    "            dim_ctx=dim_ctx,\n",
    "            dim_hidden=dim_hidden,\n",
    "            n_lstm_steps=maxlen+1, # w1~wN까지 예측한 뒤 마지막에 '.'예측해야하니까 +1\n",
    "            batch_size=batch_size,\n",
    "            ctx_shape=ctx_shape,\n",
    "            bias_init_vector=bias_init_vector)\n",
    "\n",
    "    loss, context, sentence, mask = caption_generator.build_model()\n",
    "    saver = tf.train.Saver(max_to_keep=50)\n",
    "\n",
    "    \n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    tf.initialize_all_variables().run()\n",
    "    if pretrained_model_path is not None:\n",
    "        print \"Starting with pretrained model\"\n",
    "        saver.restore(sess, pretrained_model_path)\n",
    "\n",
    "    index = annotation_data.keys()\n",
    "    #np.random.shuffle(index)\n",
    "    #annotation_data = annotation_data.ix[index]\n",
    "\n",
    "    #captions = annotation_data['caption'].values\n",
    "    image_id = annotation_data.keys()\n",
    "                                  \n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for start, end in zip( \\\n",
    "                range(0, len(captions), batch_size),\n",
    "                range(batch_size, len(captions), batch_size)):\n",
    "\n",
    "            current_feats = np.array([feats[x] for x in image_id[start:end] ])\n",
    "            current_feats = current_feats.reshape(-1, ctx_shape[1], ctx_shape[0]).swapaxes(1,2)\n",
    "\n",
    "            current_captions = captions[start:end]\n",
    "            current_caption_ind = map(lambda cap: [wordtoix[word] for word in cap.lower().split(' ') if word in wordtoix], current_captions) # '.'은 제거\n",
    "\n",
    "            current_caption_matrix = sequence.pad_sequences(current_caption_ind, padding='post', maxlen=maxlen+1)\n",
    "\n",
    "            current_mask_matrix = np.zeros((current_caption_matrix.shape[0], current_caption_matrix.shape[1]))\n",
    "            nonzeros = np.array( map(lambda x: (x != 0).sum()+1, current_caption_matrix ))\n",
    "\n",
    "            for ind, row in enumerate(current_mask_matrix):\n",
    "                row[:nonzeros[ind]] = 1\n",
    "\n",
    "            _, loss_value = sess.run([train_op, loss], feed_dict={\n",
    "                context:current_feats,\n",
    "                sentence:current_caption_matrix,\n",
    "                mask:current_mask_matrix})\n",
    "\n",
    "            print \"Current Cost: \", loss_value\n",
    "        saver.save(sess, os.path.join(model_path, 'model'), global_step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing word counts and creating vocab based on word count threshold 0\n",
      "filtered words from 1362 to 1362\n",
      "Current Cost:  5.95998\n",
      "Current Cost:  5.94182\n",
      "Current Cost:  6.08181\n",
      "Current Cost:  5.89515\n",
      "Current Cost:  5.87336\n",
      "Current Cost:  6.04251\n",
      "Current Cost:  5.83051\n",
      "Current Cost:  5.81027\n",
      "Current Cost:  5.97738\n",
      "Current Cost:  5.73983\n",
      "Current Cost:  5.73094\n",
      "Current Cost:  5.90942\n",
      "Current Cost:  5.61007\n",
      "Current Cost:  5.61181\n",
      "Current Cost:  5.83284\n",
      "Current Cost:  5.48976\n",
      "Current Cost:  5.50577\n",
      "Current Cost:  5.72968\n",
      "Current Cost:  5.35807\n",
      "Current Cost:  5.39083\n",
      "Current Cost:  5.62616\n",
      "Current Cost:  5.23496\n",
      "Current Cost:  5.28849\n",
      "Current Cost:  5.50061\n",
      "Current Cost:  5.08424\n",
      "Current Cost:  5.16502\n",
      "Current Cost:  5.42624\n",
      "Current Cost:  4.97845\n",
      "Current Cost:  5.0459\n",
      "Current Cost:  5.27823\n",
      "Current Cost:  4.79709\n",
      "Current Cost:  4.86554\n",
      "Current Cost:  5.18148\n",
      "Current Cost:  4.62536\n",
      "Current Cost:  4.73223\n",
      "Current Cost:  5.02811\n",
      "Current Cost:  4.49578\n",
      "Current Cost:  4.58808\n",
      "Current Cost:  4.89913\n",
      "Current Cost:  4.36374\n",
      "Current Cost:  4.4395\n",
      "Current Cost:  4.7526\n",
      "Current Cost:  4.15757\n",
      "Current Cost:  4.29297\n",
      "Current Cost:  4.64205\n",
      "Current Cost:  3.97815\n",
      "Current Cost:  4.13749\n",
      "Current Cost:  4.50258\n",
      "Current Cost:  3.8999\n",
      "Current Cost:  3.96775\n",
      "Current Cost:  4.41955\n",
      "Current Cost:  3.73163\n",
      "Current Cost:  3.89565\n",
      "Current Cost:  4.2278\n",
      "Current Cost:  3.57264\n",
      "Current Cost:  3.70824\n",
      "Current Cost:  4.04702\n",
      "Current Cost:  3.4147\n",
      "Current Cost:  3.55959\n",
      "Current Cost:  3.8858\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(test_feat=\"test_data/features/test_feat.p\", model_path='test_data/model/model-19'):\n",
    "    annotation_data = pickle.load(open(annotation_path,\"rb\"))\n",
    "    captions = annotation_data.values()\n",
    "    wordtoix, ixtoword, bias_init_vector = preProBuildWordVocab(captions)\n",
    "    n_words = len(wordtoix)\n",
    "    feat = np.load(test_feat)\n",
    "    feat = np.array([feat[x] for x in ['200744']])\n",
    "    feat = feat.reshape(-1, ctx_shape[1], ctx_shape[0]).swapaxes(1,2)\n",
    "    maxlen = np.max( map(lambda x: len(x.split(' ')), captions) )\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    #tf.reset_default_graph()\n",
    "\n",
    "    caption_generator = Caption_Generator(\n",
    "            n_words=n_words,\n",
    "            dim_embed=dim_embed,\n",
    "            dim_ctx=dim_ctx,\n",
    "            dim_hidden=dim_hidden,\n",
    "            n_lstm_steps=maxlen,\n",
    "            batch_size=batch_size,\n",
    "            ctx_shape=ctx_shape)\n",
    "\n",
    "    context, generated_words, logit_list, alpha_list = caption_generator.build_generator(maxlen=maxlen)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, model_path)\n",
    "    print(\"Model Restored\")\n",
    "\n",
    "    generated_word_index = sess.run(generated_words, feed_dict={context:feat})\n",
    "    #alpha_list_val = sess.run(alpha_list, feed_dict={context:feat})\n",
    "    generated_words = [ixtoword[x[0]] for x in generated_word_index]\n",
    "    #punctuation = np.argmax(np.array(generated_words) == '.')+1\n",
    "\n",
    "    #generated_words = generated_words[:punctuation]\n",
    "    #alpha_list_val = alpha_list_val[:punctuation]\n",
    "    return generated_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing word counts and creating vocab based on word count threshold 0\n",
      "filtered words from 1362 to 1362\n",
      "Model Restored\n",
      "4 pork to the center should read at least 165 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175 degrees f (175\n"
     ]
    }
   ],
   "source": [
    "generated_words = test()\n",
    "print(' '.join(generated_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annotation_data = pickle.load(open(annotation_path,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 pork chops\n",
      "1 tablespoon butter\n",
      "1/2 cup brown sugar\n",
      "3/4 cup evaporated milk\n",
      "1/4 cup light corn syrup\n",
      "1/4 cup chopped pecans\n",
      "In a large skillet over medium-high heat, cook the pork chops until no longer pink in the center, 5 to 7 minutes per side. An instant-read thermometer inserted into the center should read 145 degrees F (63 degrees C).\n",
      "Melt the butter in a saucepan; stir the brown sugar into the melted butter until smooth. Add the evaporated milk and stir; bring the mixture to a boil and immediately remove from heat. Stir the corn syrup into the milk mixture; fold the pecans into the sauce. Spoon the sauce over the pork chops to serve.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(annotation_data['200744'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
